#!/usr/bin/env ts-node

/**
 * DataVault Toplu Y√ºkleme Demonstrasyonu
 * Bu script kapsamlƒ± toplu y√ºkleme √∂rneklerini g√∂sterir
 */

import { AIRepository } from '../src/repository';
import { DatasetMetadata } from '../src/types';
import * as fs from 'fs-extra';
import * as path from 'path';
import dotenv from 'dotenv';

// Environment variables y√ºkle
dotenv.config();

async function createExampleFiles(): Promise<void> {
  console.log('üìÅ √ñrnek dosyalar olu≈üturuluyor...');
  
  const exampleDir = './examples/sample-data';
  await fs.ensureDir(exampleDir);

  // Farklƒ± t√ºrde √∂rnek dosyalar olu≈ütur
  const files = [
    // PyTorch model dosyalarƒ±
    { name: 'mnist_train.pt', content: Buffer.from('PyTorch model - train'), type: 'application/pytorch' },
    { name: 'mnist_val.pt', content: Buffer.from('PyTorch model - validation'), type: 'application/pytorch' },
    { name: 'mnist_test.pt', content: Buffer.from('PyTorch model - test'), type: 'application/pytorch' },
    
    // Veri seti dosyalarƒ±
    { name: 'train_data.json', content: JSON.stringify({ samples: 60000, features: 784 }), type: 'application/json' },
    { name: 'val_data.json', content: JSON.stringify({ samples: 10000, features: 784 }), type: 'application/json' },
    { name: 'test_data.json', content: JSON.stringify({ samples: 10000, features: 784 }), type: 'application/json' },
    
    // Embedding dosyalarƒ±
    { name: 'word_embeddings.vec', content: 'word2vec embeddings data', type: 'application/octet-stream' },
    { name: 'sentence_embeddings.npy', content: Buffer.from('numpy array embeddings'), type: 'application/octet-stream' },
    
    // Konfigurasyon dosyalarƒ±
    { name: 'training_config.json', content: JSON.stringify({ 
      learning_rate: 0.001, 
      batch_size: 32, 
      epochs: 100 
    }), type: 'application/json' },
    { name: 'model_architecture.json', content: JSON.stringify({ 
      layers: ['conv2d', 'relu', 'maxpool', 'fc'],
      params: 1234567
    }), type: 'application/json' }
  ];

  for (const file of files) {
    const filePath = path.join(exampleDir, file.name);
    await fs.writeFile(filePath, file.content);
  }

  console.log('‚úÖ √ñrnek dosyalar olu≈üturuldu!');
}

async function demonstrateBatchUpload(): Promise<void> {
  console.log('\nüöÄ Toplu Y√ºkleme Demonstrasyonu Ba≈ülƒ±yor...\n');

  const privateKey = process.env.IRYS_PRIVATE_KEY;
  const dbPath = process.env.DATABASE_PATH || './data/batch-demo.db';

  if (!privateKey) {
    console.error('‚ùå IRYS_PRIVATE_KEY environment variable gerekli');
    return;
  }

  // Repository'yi ba≈ülat
  const repository = new AIRepository(privateKey, dbPath);

  try {
    // 1. Model Dosyalarƒ± Toplu Y√ºkleme
    console.log('ü§ñ 1. Model Dosyalarƒ± Toplu Y√ºkleme');
    console.log('=====================================');
    
    const modelFiles = [
      {
        filePath: './examples/sample-data/mnist_train.pt',
        metadata: {
          app: 'batch-demo',
          contentType: 'application/pytorch',
          datasetName: 'mnist-cnn',
          split: 'train',
          version: '1.0.0',
          owner: 'ml-team@example.com',
          createdAt: new Date().toISOString()
        } as DatasetMetadata
      },
      {
        filePath: './examples/sample-data/mnist_val.pt',
        metadata: {
          app: 'batch-demo',
          contentType: 'application/pytorch',
          datasetName: 'mnist-cnn',
          split: 'validation',
          version: '1.0.0',
          owner: 'ml-team@example.com',
          createdAt: new Date().toISOString()
        } as DatasetMetadata
      },
      {
        filePath: './examples/sample-data/mnist_test.pt',
        metadata: {
          app: 'batch-demo',
          contentType: 'application/pytorch',
          datasetName: 'mnist-cnn',
          split: 'test',
          version: '1.0.0',
          owner: 'ml-team@example.com',
          createdAt: new Date().toISOString()
        } as DatasetMetadata
      }
    ];

    console.log('üì§ Model dosyalarƒ± y√ºkleniyor...');
    const modelResults = await repository.batchUpload(modelFiles, { 
      receipt: true, 
      batchSize: 2 
    });
    
    console.log('‚úÖ Model y√ºklemeleri tamamlandƒ±!');
    modelResults.forEach((result, index) => {
      console.log(`   ${index + 1}. ${result.transactionId} - ${modelFiles[index].metadata.split}`);
    });

    // 2. Veri Seti Dosyalarƒ± Toplu Y√ºkleme
    console.log('\nüìä 2. Veri Seti Dosyalarƒ± Toplu Y√ºkleme');
    console.log('========================================');
    
    const datasetFiles = [
      {
        filePath: './examples/sample-data/train_data.json',
        metadata: {
          app: 'batch-demo',
          contentType: 'application/json',
          datasetName: 'mnist-dataset',
          split: 'train',
          version: '1.0.0',
          owner: 'data-team@example.com',
          createdAt: new Date().toISOString()
        } as DatasetMetadata
      },
      {
        filePath: './examples/sample-data/val_data.json',
        metadata: {
          app: 'batch-demo',
          contentType: 'application/json',
          datasetName: 'mnist-dataset',
          split: 'validation',
          version: '1.0.0',
          owner: 'data-team@example.com',
          createdAt: new Date().toISOString()
        } as DatasetMetadata
      },
      {
        filePath: './examples/sample-data/test_data.json',
        metadata: {
          app: 'batch-demo',
          contentType: 'application/json',
          datasetName: 'mnist-dataset',
          split: 'test',
          version: '1.0.0',
          owner: 'data-team@example.com',
          createdAt: new Date().toISOString()
        } as DatasetMetadata
      }
    ];

    console.log('üì§ Veri seti dosyalarƒ± y√ºkleniyor...');
    const datasetResults = await repository.batchUpload(datasetFiles, { 
      receipt: true, 
      batchSize: 3 
    });
    
    console.log('‚úÖ Veri seti y√ºklemeleri tamamlandƒ±!');
    datasetResults.forEach((result, index) => {
      console.log(`   ${index + 1}. ${result.transactionId} - ${datasetFiles[index].metadata.split}`);
    });

    // 3. Embedding Dosyalarƒ± Toplu Y√ºkleme
    console.log('\nüîç 3. Embedding Dosyalarƒ± Toplu Y√ºkleme');
    console.log('======================================');
    
    const embeddingFiles = [
      {
        filePath: './examples/sample-data/word_embeddings.vec',
        metadata: {
          app: 'nlp-pipeline',
          contentType: 'application/octet-stream',
          datasetName: 'word-embeddings',
          split: 'production',
          version: '2.0.0',
          owner: 'nlp-team@example.com',
          createdAt: new Date().toISOString()
        } as DatasetMetadata
      },
      {
        filePath: './examples/sample-data/sentence_embeddings.npy',
        metadata: {
          app: 'nlp-pipeline',
          contentType: 'application/octet-stream',
          datasetName: 'sentence-embeddings',
          split: 'production',
          version: '1.5.0',
          owner: 'nlp-team@example.com',
          createdAt: new Date().toISOString()
        } as DatasetMetadata
      }
    ];

    console.log('üì§ Embedding dosyalarƒ± y√ºkleniyor...');
    const embeddingResults = await repository.batchUpload(embeddingFiles, { 
      receipt: true, 
      batchSize: 2 
    });
    
    console.log('‚úÖ Embedding y√ºklemeleri tamamlandƒ±!');
    embeddingResults.forEach((result, index) => {
      console.log(`   ${index + 1}. ${result.transactionId} - ${embeddingFiles[index].metadata.datasetName}`);
    });

    // 4. Konfigurasyon Dosyalarƒ± Toplu Y√ºkleme
    console.log('\n‚öôÔ∏è 4. Konfigurasyon Dosyalarƒ± Toplu Y√ºkleme');
    console.log('==========================================');
    
    const configFiles = [
      {
        filePath: './examples/sample-data/training_config.json',
        metadata: {
          app: 'batch-demo',
          contentType: 'application/json',
          datasetName: 'training-config',
          split: 'production',
          version: '1.0.0',
          owner: 'ml-engineer@example.com',
          createdAt: new Date().toISOString()
        } as DatasetMetadata
      },
      {
        filePath: './examples/sample-data/model_architecture.json',
        metadata: {
          app: 'batch-demo',
          contentType: 'application/json',
          datasetName: 'model-architecture',
          split: 'production',
          version: '1.0.0',
          owner: 'ml-architect@example.com',
          createdAt: new Date().toISOString()
        } as DatasetMetadata
      }
    ];

    console.log('üì§ Konfigurasyon dosyalarƒ± y√ºkleniyor...');
    const configResults = await repository.batchUpload(configFiles, { 
      receipt: true, 
      batchSize: 2 
    });
    
    console.log('‚úÖ Konfigurasyon y√ºklemeleri tamamlandƒ±!');
    configResults.forEach((result, index) => {
      console.log(`   ${index + 1}. ${result.transactionId} - ${configFiles[index].metadata.datasetName}`);
    });

    // 5. Toplu Sorgu Testi
    console.log('\nüîç 5. Toplu Y√ºkleme Sonu√ßlarƒ±nƒ± Sorgulama');
    console.log('========================================');
    
    const queryResults = await repository.queryData({
      filters: { app: 'batch-demo' },
      limit: 20
    });
    
    console.log(`üìä Batch-demo uygulamasƒ± i√ßin ${queryResults.results.length} sonu√ß bulundu:`);
    queryResults.results.forEach((result, index) => {
      console.log(`   ${index + 1}. ${result.tags.datasetName} (${result.tags.split}) - v${result.tags.version}`);
    });

    // 6. Performance ƒ∞statistikleri
    console.log('\nüìà 6. Performance ƒ∞statistikleri');
    console.log('===============================');
    
    const totalFiles = modelFiles.length + datasetFiles.length + embeddingFiles.length + configFiles.length;
    console.log(`‚úÖ Toplam ${totalFiles} dosya ba≈üarƒ±yla y√ºklendi`);
    console.log(`üìä Toplu i≈ülem verimli ≈üekilde tamamlandƒ±`);
    console.log(`üîç T√ºm dosyalar sorgulanabilir durumda`);

    // 7. Hesap Bakiyesi Kontrol√º
    console.log('\nüí∞ 7. Hesap Bakiyesi');
    console.log('==================');
    
    const balance = await repository.getBalance();
    console.log(`üí≥ G√ºncel bakiye: ${balance} AR`);

  } catch (error) {
    console.error('‚ùå Toplu y√ºkleme hatasƒ±:', error instanceof Error ? error.message : String(error));
  } finally {
    await repository.close();
  }
}

async function cleanupExampleFiles(): Promise<void> {
  console.log('\nüßπ √ñrnek dosyalar temizleniyor...');
  
  const exampleDir = './examples/sample-data';
  if (await fs.pathExists(exampleDir)) {
    await fs.remove(exampleDir);
    console.log('‚úÖ Temizlik tamamlandƒ±!');
  }
}

// Main execution
async function main() {
  try {
    await createExampleFiles();
    await demonstrateBatchUpload();
  } catch (error) {
    console.error('‚ùå Demo hatasƒ±:', error);
  } finally {
    await cleanupExampleFiles();
  }
}

// Script doƒürudan √ßalƒ±≈ütƒ±rƒ±lƒ±rsa main'i √ßaƒüƒ±r
if (require.main === module) {
  main();
}

export { main as batchUploadDemo };