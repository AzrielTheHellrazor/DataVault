#!/usr/bin/env ts-node

/**
 * DataVault Toplu YÃ¼kleme Demonstrasyonu
 * Bu script kapsamlÄ± toplu yÃ¼kleme Ã¶rneklerini gÃ¶sterir
 */

import { AIRepository } from '../src/repository';
import { DatasetMetadata } from '../src/types';
import * as fs from 'fs-extra';
import * as path from 'path';
import dotenv from 'dotenv';

// Environment variables yÃ¼kle
dotenv.config();

async function createExampleFiles(): Promise<void> {
  console.log('ğŸ“ Ã–rnek dosyalar oluÅŸturuluyor...');
  
  const exampleDir = './examples/sample-data';
  await fs.ensureDir(exampleDir);

  // FarklÄ± tÃ¼rde Ã¶rnek dosyalar oluÅŸtur
  const files = [
    // PyTorch model dosyalarÄ±
    { name: 'mnist_train.pt', content: Buffer.from('PyTorch model - train'), type: 'application/pytorch' },
    { name: 'mnist_val.pt', content: Buffer.from('PyTorch model - validation'), type: 'application/pytorch' },
    { name: 'mnist_test.pt', content: Buffer.from('PyTorch model - test'), type: 'application/pytorch' },
    
    // Veri seti dosyalarÄ±
    { name: 'train_data.json', content: JSON.stringify({ samples: 60000, features: 784 }), type: 'application/json' },
    { name: 'val_data.json', content: JSON.stringify({ samples: 10000, features: 784 }), type: 'application/json' },
    { name: 'test_data.json', content: JSON.stringify({ samples: 10000, features: 784 }), type: 'application/json' },
    
    // Embedding dosyalarÄ±
    { name: 'word_embeddings.vec', content: 'word2vec embeddings data', type: 'application/octet-stream' },
    { name: 'sentence_embeddings.npy', content: Buffer.from('numpy array embeddings'), type: 'application/octet-stream' },
    
    // Konfigurasyon dosyalarÄ±
    { name: 'training_config.json', content: JSON.stringify({ 
      learning_rate: 0.001, 
      batch_size: 32, 
      epochs: 100 
    }), type: 'application/json' },
    { name: 'model_architecture.json', content: JSON.stringify({ 
      layers: ['conv2d', 'relu', 'maxpool', 'fc'],
      params: 1234567
    }), type: 'application/json' }
  ];

  for (const file of files) {
    const filePath = path.join(exampleDir, file.name);
    await fs.writeFile(filePath, file.content);
  }

  console.log('âœ… Ã–rnek dosyalar oluÅŸturuldu!');
}

async function demonstrateBatchUpload(): Promise<void> {
  console.log('\nğŸš€ Toplu YÃ¼kleme Demonstrasyonu BaÅŸlÄ±yor...\n');

  const privateKey = process.env.IRYS_PRIVATE_KEY;
  const dbPath = process.env.DATABASE_PATH || './data/batch-demo.db';

  if (!privateKey) {
    console.error('âŒ IRYS_PRIVATE_KEY environment variable gerekli');
    return;
  }

  // Repository'yi baÅŸlat
  const repository = new AIRepository(privateKey, dbPath);

  try {
    // 1. Model DosyalarÄ± Toplu YÃ¼kleme
    console.log('ğŸ¤– 1. Model DosyalarÄ± Toplu YÃ¼kleme');
    console.log('=====================================');
    
    const modelFiles = [
      {
        filePath: './examples/sample-data/mnist_train.pt',
        metadata: {
          app: 'batch-demo',
          contentType: 'application/pytorch',
          datasetName: 'mnist-cnn',
          split: 'train',
          version: '1.0.0',
          owner: 'ml-team@example.com',
          createdAt: new Date().toISOString()
        } as DatasetMetadata
      },
      {
        filePath: './examples/sample-data/mnist_val.pt',
        metadata: {
          app: 'batch-demo',
          contentType: 'application/pytorch',
          datasetName: 'mnist-cnn',
          split: 'validation',
          version: '1.0.0',
          owner: 'ml-team@example.com',
          createdAt: new Date().toISOString()
        } as DatasetMetadata
      },
      {
        filePath: './examples/sample-data/mnist_test.pt',
        metadata: {
          app: 'batch-demo',
          contentType: 'application/pytorch',
          datasetName: 'mnist-cnn',
          split: 'test',
          version: '1.0.0',
          owner: 'ml-team@example.com',
          createdAt: new Date().toISOString()
        } as DatasetMetadata
      }
    ];

    console.log('ğŸ“¤ Model dosyalarÄ± yÃ¼kleniyor...');
    const modelResults = await repository.batchUpload(modelFiles, { 
      receipt: true, 
      batchSize: 2 
    });
    
    console.log('âœ… Model yÃ¼klemeleri tamamlandÄ±!');
    modelResults.forEach((result, index) => {
      console.log(`   ${index + 1}. ${result.transactionId} - ${modelFiles[index].metadata.split}`);
    });

    // 2. Veri Seti DosyalarÄ± Toplu YÃ¼kleme
    console.log('\nğŸ“Š 2. Veri Seti DosyalarÄ± Toplu YÃ¼kleme');
    console.log('========================================');
    
    const datasetFiles = [
      {
        filePath: './examples/sample-data/train_data.json',
        metadata: {
          app: 'batch-demo',
          contentType: 'application/json',
          datasetName: 'mnist-dataset',
          split: 'train',
          version: '1.0.0',
          owner: 'data-team@example.com',
          createdAt: new Date().toISOString()
        } as DatasetMetadata
      },
      {
        filePath: './examples/sample-data/val_data.json',
        metadata: {
          app: 'batch-demo',
          contentType: 'application/json',
          datasetName: 'mnist-dataset',
          split: 'validation',
          version: '1.0.0',
          owner: 'data-team@example.com',
          createdAt: new Date().toISOString()
        } as DatasetMetadata
      },
      {
        filePath: './examples/sample-data/test_data.json',
        metadata: {
          app: 'batch-demo',
          contentType: 'application/json',
          datasetName: 'mnist-dataset',
          split: 'test',
          version: '1.0.0',
          owner: 'data-team@example.com',
          createdAt: new Date().toISOString()
        } as DatasetMetadata
      }
    ];

    console.log('ğŸ“¤ Veri seti dosyalarÄ± yÃ¼kleniyor...');
    const datasetResults = await repository.batchUpload(datasetFiles, { 
      receipt: true, 
      batchSize: 3 
    });
    
    console.log('âœ… Veri seti yÃ¼klemeleri tamamlandÄ±!');
    datasetResults.forEach((result, index) => {
      console.log(`   ${index + 1}. ${result.transactionId} - ${datasetFiles[index].metadata.split}`);
    });

    // 3. Embedding DosyalarÄ± Toplu YÃ¼kleme
    console.log('\nğŸ” 3. Embedding DosyalarÄ± Toplu YÃ¼kleme');
    console.log('======================================');
    
    const embeddingFiles = [
      {
        filePath: './examples/sample-data/word_embeddings.vec',
        metadata: {
          app: 'nlp-pipeline',
          contentType: 'application/octet-stream',
          datasetName: 'word-embeddings',
          split: 'production',
          version: '2.0.0',
          owner: 'nlp-team@example.com',
          createdAt: new Date().toISOString()
        } as DatasetMetadata
      },
      {
        filePath: './examples/sample-data/sentence_embeddings.npy',
        metadata: {
          app: 'nlp-pipeline',
          contentType: 'application/octet-stream',
          datasetName: 'sentence-embeddings',
          split: 'production',
          version: '1.5.0',
          owner: 'nlp-team@example.com',
          createdAt: new Date().toISOString()
        } as DatasetMetadata
      }
    ];

    console.log('ğŸ“¤ Embedding dosyalarÄ± yÃ¼kleniyor...');
    const embeddingResults = await repository.batchUpload(embeddingFiles, { 
      receipt: true, 
      batchSize: 2 
    });
    
    console.log('âœ… Embedding yÃ¼klemeleri tamamlandÄ±!');
    embeddingResults.forEach((result, index) => {
      console.log(`   ${index + 1}. ${result.transactionId} - ${embeddingFiles[index].metadata.datasetName}`);
    });

    // 4. Konfigurasyon DosyalarÄ± Toplu YÃ¼kleme
    console.log('\nâš™ï¸ 4. Konfigurasyon DosyalarÄ± Toplu YÃ¼kleme');
    console.log('==========================================');
    
    const configFiles = [
      {
        filePath: './examples/sample-data/training_config.json',
        metadata: {
          app: 'batch-demo',
          contentType: 'application/json',
          datasetName: 'training-config',
          split: 'production',
          version: '1.0.0',
          owner: 'ml-engineer@example.com',
          createdAt: new Date().toISOString()
        } as DatasetMetadata
      },
      {
        filePath: './examples/sample-data/model_architecture.json',
        metadata: {
          app: 'batch-demo',
          contentType: 'application/json',
          datasetName: 'model-architecture',
          split: 'production',
          version: '1.0.0',
          owner: 'ml-architect@example.com',
          createdAt: new Date().toISOString()
        } as DatasetMetadata
      }
    ];

    console.log('ğŸ“¤ Konfigurasyon dosyalarÄ± yÃ¼kleniyor...');
    const configResults = await repository.batchUpload(configFiles, { 
      receipt: true, 
      batchSize: 2 
    });
    
    console.log('âœ… Konfigurasyon yÃ¼klemeleri tamamlandÄ±!');
    configResults.forEach((result, index) => {
      console.log(`   ${index + 1}. ${result.transactionId} - ${configFiles[index].metadata.datasetName}`);
    });

    // 5. Toplu Sorgu Testi
    console.log('\nğŸ” 5. Toplu YÃ¼kleme SonuÃ§larÄ±nÄ± Sorgulama');
    console.log('========================================');
    
    const queryResults = await repository.queryData({
      filters: { app: 'batch-demo' },
      limit: 20
    });
    
    console.log(`ğŸ“Š Batch-demo uygulamasÄ± iÃ§in ${queryResults.results.length} sonuÃ§ bulundu:`);
    queryResults.results.forEach((result, index) => {
      console.log(`   ${index + 1}. ${result.tags.datasetName} (${result.tags.split}) - v${result.tags.version}`);
    });

    // 6. Performance Ä°statistikleri
    console.log('\nğŸ“ˆ 6. Performance Ä°statistikleri');
    console.log('===============================');
    
    const totalFiles = modelFiles.length + datasetFiles.length + embeddingFiles.length + configFiles.length;
    console.log(`âœ… Toplam ${totalFiles} dosya baÅŸarÄ±yla yÃ¼klendi`);
    console.log(`ğŸ“Š Toplu iÅŸlem verimli ÅŸekilde tamamlandÄ±`);
    console.log(`ğŸ” TÃ¼m dosyalar sorgulanabilir durumda`);

    // 7. Hesap Bakiyesi KontrolÃ¼
    console.log('\nğŸ’° 7. Hesap Bakiyesi');
    console.log('==================');
    
    const balance = await repository.getBalance();
    console.log(`ğŸ’³ GÃ¼ncel bakiye: ${balance} AR`);

  } catch (error) {
    console.error('âŒ Toplu yÃ¼kleme hatasÄ±:', error instanceof Error ? error.message : String(error));
  } finally {
    await repository.close();
  }
}

async function cleanupExampleFiles(): Promise<void> {
  console.log('\nğŸ§¹ Ã–rnek dosyalar temizleniyor...');
  
  const exampleDir = './examples/sample-data';
  if (await fs.pathExists(exampleDir)) {
    await fs.remove(exampleDir);
    console.log('âœ… Temizlik tamamlandÄ±!');
  }
}

// Main execution
async function main() {
  try {
    await createExampleFiles();
    await demonstrateBatchUpload();
  } catch (error) {
    console.error('âŒ Demo hatasÄ±:', error);
  } finally {
    await cleanupExampleFiles();
  }
}

// Script doÄŸrudan Ã§alÄ±ÅŸtÄ±rÄ±lÄ±rsa main'i Ã§aÄŸÄ±r
if (require.main === module) {
  main();
}

export { main as batchUploadDemo };